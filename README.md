# Setup

You'll need:
* A kubernetes cluster (and a name for the cluster)
* SUSE Observability with its:
  * OTLP endpoint
  * A receiver api key

Create a namespace:
```
kubectl create namespace suse-observability
```

Create a secret with the SUSE Observabilty api key:

```
kubectl create secret generic open-telemetry-collector \
  --namespace suse-observability \
  --from-literal=API_KEY='<api-key>'
```

## Installing the operator

For production installations update the values.yaml to use certmanager instead of a certificate generated by Helm during installation (set `admissionWebhooks.certManager.enabled=true`).

When using a private image registry customize the repositories and add an optional image pull secret (see values.yaml) for the operator. 

Creating the secret:

```
kubectl create secret docker-registry <secret-name> --docker-server=<registry name> \
        --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \
        --docker-email=DUMMY_DOCKER_EMAIL
```

Install the operator with:

```
./install_operator.sh
```

## Installing the collector

Edit the collector.yaml to set your:
* Cluster name - Replace `value: demo-cluster` with the name of your k8s cluster `value: my-cluster-name`
* SUSE Observability URL: Replace `endpoint: https://<suse-observability>:443` with your SUSE Observability OTLP endpoint (for example `https://otlp-my-suse-observability.mydomain.com:443`)

If you need to use an image pull secret for your private registry create a service account for the collector, assign the imagepullsecret to the service account.

```
kubectl create serviceaccount otel-collector
kubectl patch serviceaccount otel-collector -p '{"imagePullSecrets": [{"name": "<secret-name>"}]}'
```

Edit the collector.yaml to include the optional service account

Install the collector (it will be installed in the suse-observability namespace):
```
kubectl apply -f collector.yaml
```

## Configure instrumentation

Apply the `instrumentation.yaml` **in the namespace of the application you want to auto-instrument**:

```
kubectl create namespace demo
kubectl apply -f instrumentation.yaml --namespace demo
```

Now deploy the dice-roller demo:

```
cd dice-roller-app
kubectl apply --namespace demo -f dice-roller.yaml
```

To test that traces are being sent to SUSE Observability we need to generate some traffic to the application:

```
kubectl port-forward -n demo service/dice-roller 5000:5000 &
while true; do; curl localhost:5000/role; sleep 3; done 
```

## Instrumenting your own apps

Add an annotation to the pods that need to be auto-instrumented (This is a copy directly from the README at https://github.com/open-telemetry/opentelemetry-operator#opentelemetry-auto-instrumentation-injection). See the `dice-roller.yaml` for an example with Python.

If you're using a private registry make sure you're pod has an imagePullSecret set that allows it to pull from the private registry

Java:

```bash
instrumentation.opentelemetry.io/inject-java: "true"
```

NodeJS:

```bash
instrumentation.opentelemetry.io/inject-nodejs: "true"
```

Python:
Python auto-instrumentation also honors an annotation that will permit it to run it on images with a different C library than glibc.

```bash
instrumentation.opentelemetry.io/inject-python: "true"
instrumentation.opentelemetry.io/otel-python-platform: "glibc" # for Linux glibc based images, this is the default value and can be omitted
instrumentation.opentelemetry.io/otel-python-platform: "musl" # for Linux musl based images
```

.NET:
.NET auto-instrumentation also honors an annotation that will be used to set the .NET [Runtime Identifiers](https://learn.microsoft.com/en-us/dotnet/core/rid-catalog)(RIDs).
Currently, only two RIDs are supported: `linux-x64` and `linux-musl-x64`.
By default `linux-x64` is used.

```bash
instrumentation.opentelemetry.io/inject-dotnet: "true"
instrumentation.opentelemetry.io/otel-dotnet-auto-runtime: "linux-x64" # for Linux glibc based images, this is default value and can be omitted
instrumentation.opentelemetry.io/otel-dotnet-auto-runtime: "linux-musl-x64"  # for Linux musl based images
```

Go:

Go auto-instrumentation also honors an annotation that will be used to set the [OTEL_GO_AUTO_TARGET_EXE env var](https://github.com/open-telemetry/opentelemetry-go-instrumentation/blob/main/docs/how-it-works.md).
This env var can also be set via the Instrumentation resource, with the annotation taking precedence.
Since Go auto-instrumentation requires `OTEL_GO_AUTO_TARGET_EXE` to be set, you must supply a valid
executable path via the annotation or the Instrumentation resource. Failure to set this value causes instrumentation injection to abort, leaving the original pod unchanged.

```bash
instrumentation.opentelemetry.io/inject-go: "true"
instrumentation.opentelemetry.io/otel-go-auto-target-exe: "/path/to/container/executable"
```

Go auto-instrumentation also requires elevated permissions. The below permissions are set automatically and are required.

```yaml
securityContext:
  privileged: true
  runAsUser: 0
```

The possible values for the annotation can be

- `"true"` - inject and `Instrumentation` resource from the namespace.
- `"my-instrumentation"` - name of `Instrumentation` CR instance in the current namespace.
- `"my-other-namespace/my-instrumentation"` - name and namespace of `Instrumentation` CR instance in another namespace.
- `"false"` - do not inject

When your pod has multiple containers that should be instrumented (not just the first) that is also possible. See https://github.com/open-telemetry/opentelemetry-operator?tab=readme-ov-file#multi-container-pods-with-multiple-instrumentations.